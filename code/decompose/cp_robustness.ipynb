{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP Decomposition Robustness Analysis\n",
    "\n",
    "This notebook demonstrates how to use the CP decomposition functionality from `decompose_cp.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Adjust the path as needed to reach your project root from the notebook's location\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PROJECT_ROOT\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 18:23:37,520 - INFO - Running on local environment\n",
      "2025-07-21 18:23:37,521 - INFO - Using random state: 42\n",
      "2025-07-21 18:23:37,596 - INFO - Using mps device for computation\n",
      "2025-07-21 18:23:37,596 - INFO - Using Apple MPS (Metal) for acceleration - memory stats not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment - packages should be installed in local environment\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import tensorly as tl\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations, chain\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "import torch\n",
    "\n",
    "# # Add the code directory to the path to import decompose_cp\n",
    "# sys.path.append(str(Path.cwd() / 'code'))\n",
    "\n",
    "# Import functions from decompose_cp\n",
    "from decompose_cp import (\n",
    "    load_sparse_tensor,\n",
    "    run_cp_decomposition,\n",
    "    compute_c1_unique_zones,\n",
    "    compute_c2_unique_od_pairs,\n",
    "    get_device,\n",
    "    parse_rank_range\n",
    ")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configure TensorLy to use PyTorch backend\n",
    "tl.set_backend('pytorch')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Results will be saved to: /Users/peterfalterbaum/Documents/Nova/thesis local/implementation/public_implementation/data/results/decompositions/robustness/CP\n"
     ]
    }
   ],
   "source": [
    "tensor_paths = [\n",
    "    (5, str(PROJECT_ROOT) + \"/data/processed/utrecht/odt_no_same_od_no_rare_od_fixed_thresh_normalizedPeaks/timehours/odt_processed_utrecht_hourly_weekday.npz\"),\n",
    "    (5, str(PROJECT_ROOT) + \"/data/processed/utrecht/odt_no_same_od_no_rare_od_fixed_thresh_normalizedPeaks/timehours/odt_processed_utrecht_hourly_weekend.npz\"),\n",
    "    (6, str(PROJECT_ROOT) + \"/data/processed/rotterdam/odt_no_same_od_no_rare_od_fixed_thresh_normalizedPeaks/timehours/odt_processed_rotterdam_hourly_weekday.npz\"),\n",
    "    (6, str(PROJECT_ROOT) + \"/data/processed/rotterdam/odt_no_same_od_no_rare_od_fixed_thresh_normalizedPeaks/timehours/odt_processed_rotterdam_hourly_weekend.npz\"),\n",
    "]\n",
    "\n",
    "results_path = str(PROJECT_ROOT) + \"/data/results/decompositions/robustness/CP\"\n",
    "\n",
    "analysis_dir = None\n",
    "\n",
    "\n",
    "# Get device information\n",
    "device, device_name = get_device()\n",
    "print(f\"Using device: {device_name}\")\n",
    "print(f\"Results will be saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cp_robustness_analysis(tensor_path: str, ranks: list[int],\n",
    "                               num_runs: int = 5, optimizer: str = 'ALS',\n",
    "                               max_iter: int = 1000, tol: float = 1e-8,\n",
    "                               init_methods_param: str = 'random', l1_reg: float = 0.0,\n",
    "                               l2_reg: float = 0.0, top_n: int = 2,\n",
    "                               use_percentage: bool = True, percentage: float = 0.2) -> None:\n",
    "    \"\"\"\n",
    "    Run CP decomposition robustness analysis with multiple random seeds.\n",
    "    \"\"\"\n",
    "    # Create results directory\n",
    "    results_dir = Path(results_path)\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create timestamped directory for this analysis\n",
    "    global analysis_dir\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    identifier_tensor = \"_\".join(Path(tensor_path).stem.split(\"_\")[-3:])\n",
    "    analysis_dir = results_dir / f\"{identifier_tensor}/{timestamp}\"\n",
    "    analysis_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(f\"Starting robustness analysis with {num_runs} runs\")\n",
    "    logger.info(f\"Results will be stored in: {analysis_dir}\")\n",
    "\n",
    "    # Load the tensor\n",
    "    logger.info(f\"Loading tensor from: {tensor_path}\")\n",
    "    tensor, origins, destinations, time_indices = load_sparse_tensor(\n",
    "        tensor_path)\n",
    "\n",
    "    # Extract tensor name from the path\n",
    "    tensor_name = Path(tensor_path).stem\n",
    "\n",
    "    # Run multiple decompositions with different random seeds\n",
    "    for run_num in range(num_runs):\n",
    "        logger.info(f\"\\n=== Starting Run {run_num + 1}/{num_runs} ===\")\n",
    "\n",
    "        # Generate a different random seed for each run\n",
    "        random_seed = np.random.randint(0, 1000000)\n",
    "        logger.info(f\"Using random seed: {random_seed}\")\n",
    "\n",
    "        # Create run-specific directory\n",
    "        run_dir = analysis_dir / f\"run_{run_num + 1:03d}_seed_{random_seed}\"\n",
    "\n",
    "        # Use SVD initialization for the first run as baseline\n",
    "        if run_num == 0:\n",
    "            init_methods = 'svd'\n",
    "            logger.info(f\"Using SVD initialization for the first run\")\n",
    "            run_dir = analysis_dir / \\\n",
    "                f\"svd_{run_num + 1:03d}_seed_{random_seed}\"\n",
    "        else:\n",
    "            init_methods = init_methods_param\n",
    "\n",
    "        run_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            # Run CP decomposition\n",
    "            results_dict, run_results_dir = run_cp_decomposition(\n",
    "                input_tensor=tensor,\n",
    "                ranks=ranks,\n",
    "                tensor_name=tensor_name,\n",
    "                output_path=run_dir,\n",
    "                optimizer=optimizer,\n",
    "                max_iter=max_iter,\n",
    "                tol=tol,\n",
    "                init_methods=init_methods,\n",
    "                l1_reg=l1_reg,\n",
    "                l2_reg=l2_reg,\n",
    "                top_n=top_n,\n",
    "                use_percentage=use_percentage,\n",
    "                percentage=percentage,\n",
    "                origins=origins,\n",
    "                destinations=destinations,\n",
    "                random_state=random_seed\n",
    "            )\n",
    "\n",
    "            # Save run-specific information\n",
    "            run_info = {\n",
    "                \"run_number\": run_num + 1,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"tensor_path\": tensor_path,\n",
    "                \"ranks\": ranks,\n",
    "                \"optimizer\": optimizer,\n",
    "                \"status\": \"completed\"\n",
    "            }\n",
    "\n",
    "            with open(run_dir / \"run_info.json\", 'w') as f:\n",
    "                json.dump(run_info, f, indent=4)\n",
    "\n",
    "            logger.info(f\"Run {run_num + 1} completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in run {run_num + 1}: {str(e)}\")\n",
    "\n",
    "            # Save error information\n",
    "            error_info = {\n",
    "                \"run_number\": run_num + 1,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"failed\"\n",
    "            }\n",
    "\n",
    "            with open(run_dir / \"error_info.json\", 'w') as f:\n",
    "                json.dump(error_info, f, indent=4)\n",
    "\n",
    "    logger.info(\n",
    "        f\"\\nRobustness analysis completed. Results stored in: {analysis_dir}\")\n",
    "\n",
    "    # Create summary file\n",
    "    summary = {\n",
    "        \"analysis_timestamp\": timestamp,\n",
    "        \"tensor_path\": tensor_path,\n",
    "        \"tensor_name\": tensor_name,\n",
    "        \"num_runs\": num_runs,\n",
    "        \"ranks\": ranks,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"analysis_dir\": str(analysis_dir),\n",
    "        \"status\": \"completed\"\n",
    "    }\n",
    "\n",
    "    with open(analysis_dir / \"analysis_summary.json\", 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    return analysis_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_robustness_analysis(tensor_path, num_runs, ranks):\n",
    "    \"\"\"Run robustness analysis on the first tensor with example parameters.\"\"\"\n",
    "\n",
    "    analysis_dir = run_cp_robustness_analysis(\n",
    "        tensor_path=tensor_path,\n",
    "        ranks=ranks,\n",
    "        num_runs=num_runs,  # Start with 3 runs for testing\n",
    "        optimizer='ALS',\n",
    "        max_iter=1000,\n",
    "        tol=1e-8,\n",
    "        init_methods_param='random',\n",
    "        l1_reg=0.0,\n",
    "        l2_reg=0.0\n",
    "    )\n",
    "\n",
    "    print(f\"Analysis completed. Results in: {analysis_dir}\")\n",
    "    return analysis_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tlviz.factor_tools import (\n",
    "    normalise_cp_tensor,\n",
    "    factor_match_score,\n",
    "    degeneracy_score\n",
    ")\n",
    "from tensorly.cp_tensor import cp_to_tensor\n",
    "import tensorly as tl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "# Increase the recursion depth limit to handle deep import chains in libraries.\n",
    "sys.setrecursionlimit(10_000)\n",
    "\n",
    "\n",
    "def load_cp_runs(analysis_dir: Path, rank: int):\n",
    "    \"\"\"\n",
    "    Load CP decomposition runs from a robustness directory.\n",
    "    Returns a list of tuples: (run_name, error, cp_tensor).\n",
    "    \"\"\"\n",
    "    runs = []\n",
    "    for run_dir in chain(analysis_dir.glob(\"run_*\"), analysis_dir.glob(\"svd_*\")):\n",
    "        for rank_dir in run_dir.rglob(f\"rank_{rank}\"):\n",
    "            metrics_file = next(rank_dir.glob(\"*_metrics.json\"), None)\n",
    "            factors_file = next(rank_dir.glob(\"*_factors.npz\"), None)\n",
    "            if metrics_file and factors_file:\n",
    "                with open(metrics_file, \"r\") as f:\n",
    "                    metrics = json.load(f)\n",
    "                data = np.load(factors_file, allow_pickle=True)\n",
    "                weights = data.get(\"weights\", data[data.files[0]])\n",
    "                factors = data.get(\"factors\", data[data.files[1]])\n",
    "                error = metrics.get(\"normalized_frobenius_error\", np.nan)\n",
    "                runs.append((run_dir.name, error, (weights, factors)))\n",
    "    return runs\n",
    "\n",
    "\n",
    "def compare_cp_stability_tlviz(analysis_dir: Path, rank: int, num_best: int = 5):\n",
    "    \"\"\"\n",
    "    Compute CP stability metrics using TLViz tools.\n",
    "\n",
    "    This function calculates and visualizes:\n",
    "      1) Pairwise Factor-Match Score\n",
    "      2) Pairwise Tensor-Cosine Similarity\n",
    "      3) Degeneracy score for each model\n",
    "\n",
    "    Returns a dict with DataFrames for the two pairwise metrics and a dict for degeneracy.\n",
    "    \"\"\"\n",
    "    # 1) Load all runs and select the best ones for comparison\n",
    "    all_runs = load_cp_runs(analysis_dir, rank)\n",
    "    if not all_runs:\n",
    "        print(f\"No runs found for rank {rank}\")\n",
    "        return {}\n",
    "\n",
    "    # Sort by error to find the best models\n",
    "    best_runs = sorted(all_runs, key=lambda x: x[1])[:num_best]\n",
    "\n",
    "    # Ensure the SVD run is included for baseline comparison if it's not already in the best runs\n",
    "    svd_run = next((r for r in all_runs if r[0].startswith(\"svd\")), None)\n",
    "    if svd_run and svd_run[0] not in [r[0] for r in best_runs]:\n",
    "        best_runs.append(svd_run)\n",
    "\n",
    "    # 3) Normalize each CP tensor and store it in a dictionary\n",
    "    models = {name: normalise_cp_tensor(cp) for name, _, cp in best_runs}\n",
    "    model_names = list(models.keys())\n",
    "\n",
    "    # 4a) Compute pairwise Factor Match Score\n",
    "    fms_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "    for i, name1 in enumerate(model_names):\n",
    "        for j, name2 in enumerate(model_names):\n",
    "            if i >= j:\n",
    "                score = factor_match_score(models[name1], models[name2])\n",
    "                fms_matrix[i, j] = score\n",
    "                fms_matrix[j, i] = score\n",
    "    df_fms = pd.DataFrame(fms_matrix, index=model_names, columns=model_names)\n",
    "\n",
    "    # 4b) Compute pairwise Tensor Cosine Similarity\n",
    "    reconstructed_models = {name: cp_to_tensor(\n",
    "        cp) for name, cp in models.items()}\n",
    "    cos_matrix = np.zeros((len(model_names), len(model_names)))\n",
    "    for i, name1 in enumerate(model_names):\n",
    "        for j, name2 in enumerate(model_names):\n",
    "            if i >= j:\n",
    "                t1 = reconstructed_models[name1]\n",
    "                t2 = reconstructed_models[name2]\n",
    "                score = tl.sum(t1 * t2) / (tl.norm(t1) * tl.norm(t2))\n",
    "                cos_matrix[i, j] = score\n",
    "                cos_matrix[j, i] = score\n",
    "    df_cos = pd.DataFrame(cos_matrix, index=model_names, columns=model_names)\n",
    "\n",
    "    # 4c) Compute Degeneracy for each model\n",
    "    deg = {name: float(degeneracy_score(cp)) for name, cp in models.items()}\n",
    "\n",
    "    # 5) Plot the results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(22, 9))\n",
    "    fig.suptitle(f\"CP Stability Analysis for Rank {rank}\", fontsize=16)\n",
    "\n",
    "    sns.heatmap(df_fms, annot=True, fmt=\".3f\", cmap=\"viridis\", ax=axes[0])\n",
    "    axes[0].set_title(\"Factor-Match Score\")\n",
    "    axes[0].tick_params(axis='x', rotation=45, ha='right')\n",
    "    axes[0].tick_params(axis='y', rotation=0)\n",
    "\n",
    "    sns.heatmap(df_cos, annot=True, fmt=\".3f\", cmap=\"viridis\", ax=axes[1])\n",
    "    axes[1].set_title(\"Tensor Cosine Similarity\")\n",
    "    axes[1].tick_params(axis='x', rotation=45, ha='right')\n",
    "    axes[1].tick_params(axis='y', rotation=0)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # Save the figure\n",
    "    out_png = analysis_dir / f\"stability_summary_rank_{rank}.png\"\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"\\nSaved stability plot to {out_png}\")\n",
    "\n",
    "    # Print degeneracy scores\n",
    "    print(f\"Degeneracy scores for Rank {rank}:\")\n",
    "    for name, score in sorted(deg.items()):\n",
    "        print(f\"  - {name}: {score:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"factor_match\": df_fms.to_dict(),\n",
    "        \"tensor_cosine\": df_cos.to_dict(),\n",
    "        \"degeneracy\": deg,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_stability_results = []\n",
    "\n",
    "for rank, tensor_path in tensor_paths:\n",
    "    # 1) kick off the robustness job *and* capture its output directory\n",
    "    result_dir = run_robustness_analysis(tensor_path, 5, [rank])\n",
    "    #    ^-- this should be a Path pointing to \"…/20250626_221737\" etc.\n",
    "\n",
    "    # 2) now pass *that* directory into your comparer\n",
    "    stability_results = compare_cp_stability_tlviz(\n",
    "        result_dir, rank=rank, num_best=5)\n",
    "    collected_stability_results.append(stability_results)\n",
    "\n",
    "# 3) finally write out your collected stats\n",
    "with open('stability_results.json', 'w') as f:\n",
    "    json.dump(collected_stability_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## individual decomposition test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/peterfalterbaum/Documents/Nova/thesis local/implementation/rocket riding/data/processed/rotterdam/odt_no_same_od_no_rare_od_fixed_thresh_normalizedPeaks/timehours/odt_processed_rotterdam_hourly_weekend.npz'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _, tensor_path = tensor_paths[3]\n",
    "# tensor_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank = 4\n",
    "# run_robustness_analysis(tensor_path, 30, [rank])\n",
    "# # Example usage:\n",
    "# # analysis_dir = Path('/Users/peterfalterbaum/Documents/Nova/thesis local/implementation/rocket riding/data/results/decompositions/robustness/CP/robustness_analysis_20250625_132752')\n",
    "# stability_results = compare_cp_stability(\n",
    "#     Path(analysis_dir), rank=rank, num_best=7)  # , tensor_path=Path(tensor_path)\n",
    "# collected_stability_results.append(stability_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stability_results = compare_cp_stability(Path(analysis_dir), rank=rank, num_best=7)  # , tensor_path=Path(tensor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocketRiding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
